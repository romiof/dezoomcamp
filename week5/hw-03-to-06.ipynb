{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ab56499",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e8acee-9219-46f4-bc37-20bd8717f8cc",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load DF from parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "247c29b5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/04 08:55:36 WARN Utils: Your hostname, FRANCIS-PC resolves to a loopback address: 127.0.1.1; using 172.31.24.92 instead (on interface eth0)\n",
      "23/03/04 08:55:36 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/04 08:55:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/03/04 08:55:38 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[12]\") \\\n",
    "    .appName('hw-03') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7c12b00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .parquet('/home/fromio/cursos/de-zoom-camp/week5/parq_files/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8201b6c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|dispatching_base_num|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|SR_Flag|Affiliated_base_number|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|              B02510|2021-06-04 08:13:36|2021-06-04 08:47:58|          91|          65|      N|                  null|\n",
      "|              B02875|2021-06-25 01:52:01|2021-06-25 02:14:51|         132|         255|      N|                B02875|\n",
      "|              B02875|2021-06-25 03:34:13|2021-06-25 03:42:22|          75|         263|      N|                B02875|\n",
      "|              B02866|2021-06-05 09:21:57|2021-06-05 09:35:38|         233|         186|      N|                B02866|\n",
      "|              B02876|2021-06-01 01:26:49|2021-06-01 01:33:02|         182|         212|      N|                B02876|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c06df1da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14961892"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2240afd2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- SR_Flag: string (nullable = true)\n",
      " |-- Affiliated_base_number: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47efccaf-da69-492e-9bea-d3c1d71465d2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4544166c-8da3-4df8-8a2d-dbdae0d1aa37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "452470"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using dataframe methods\n",
    "df.filter(df.pickup_datetime >= '2021-06-15').filter(df.pickup_datetime < '2021-06-16').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf29f8f2-77f6-4768-98d6-ca74eb2e220b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14961892"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with SparkQL\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dbe8ced1-dce3-46cd-a046-87edf9b2d20d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"fhvhv_2021_06\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "530a2384-7e87-454b-a770-e732a0ecd065",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- SR_Flag: string (nullable = true)\n",
      " |-- Affiliated_base_number: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT *\n",
    "               FROM fhvhv_2021_06 \"\"\") \\\n",
    "     .printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d7dae647-e891-49d2-9892-3a6981ca9b4b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|  452470|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT COUNT(1) \n",
    "               FROM fhvhv_2021_06\n",
    "              WHERE DAY(pickup_datetime) = 15\n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4296cc-0af4-458e-9ee2-0025531a89e8",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "Longest trip for each day\n",
    "\n",
    "Now calculate the duration for each trip.<br>\n",
    "How long was the longest trip in Hours?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "957dd9b4-a052-4e2d-b600-e0dcc2647e87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "018e06da-e3d5-4294-acee-0c063865de61",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/04 11:18:34 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/03/04 11:18:34 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/03/04 11:18:34 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/03/04 11:18:34 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/03/04 11:18:34 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/03/04 11:18:35 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/03/04 11:18:35 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/03/04 11:18:35 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/03/04 11:18:35 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/03/04 11:18:35 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/03/04 11:18:35 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/03/04 11:18:35 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 102:>                                                      (0 + 12) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/04 11:18:35 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/03/04 11:18:35 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/03/04 11:18:35 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/03/04 11:18:35 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/03/04 11:18:35 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/03/04 11:18:35 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/03/04 11:18:35 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/03/04 11:18:35 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/03/04 11:18:35 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/03/04 11:18:35 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/03/04 11:18:35 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/03/04 11:18:35 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/03/04 11:18:35 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/03/04 11:18:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/03/04 11:18:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/03/04 11:18:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/03/04 11:18:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/03/04 11:18:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/03/04 11:18:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/03/04 11:18:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/03/04 11:18:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/03/04 11:18:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/03/04 11:18:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/03/04 11:18:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/03/04 11:18:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/03/04 11:18:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/03/04 11:18:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/03/04 11:18:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/03/04 11:18:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/03/04 11:18:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/03/04 11:18:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/03/04 11:18:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/03/04 11:18:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/03/04 11:18:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/03/04 11:18:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/03/04 11:18:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 102:==================>                                     (4 + 8) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------------+\n",
      "|    pickup_datetime|max(duration_sec)|\n",
      "+-------------------+-----------------+\n",
      "|2021-06-25 13:55:41|           240764|\n",
      "|2021-06-22 12:09:45|            91979|\n",
      "|2021-06-27 10:32:29|            71931|\n",
      "|2021-06-26 22:37:11|            65510|\n",
      "|2021-06-23 20:40:43|            59281|\n",
      "|2021-06-23 22:03:31|            51368|\n",
      "|2021-06-24 23:11:00|            50075|\n",
      "|2021-06-04 20:56:02|            42012|\n",
      "|2021-06-27 07:45:19|            40917|\n",
      "|2021-06-20 17:05:12|            39544|\n",
      "|2021-06-01 12:25:29|            36963|\n",
      "|2021-06-28 13:13:59|            35879|\n",
      "|2021-06-01 12:01:46|            35879|\n",
      "|2021-06-27 03:52:14|            34696|\n",
      "|2021-06-18 08:50:29|            34648|\n",
      "|2021-06-08 16:38:14|            34129|\n",
      "|2021-06-11 23:26:20|            34098|\n",
      "|2021-06-15 06:47:22|            33848|\n",
      "|2021-06-25 02:32:24|            33817|\n",
      "|2021-06-04 17:41:23|            33757|\n",
      "+-------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df \\\n",
    "   .withColumn(\"duration_sec\", df.dropoff_datetime.cast(\"long\") - df.pickup_datetime.cast(\"long\") ) \\\n",
    "   .groupBy('pickup_datetime') \\\n",
    "   .max('duration_sec') \\\n",
    "   .orderBy('max(duration_sec)', ascending=False) \\\n",
    "   .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "39ec64f5-6a2d-47cd-91fa-a9c0de22d0c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 100:>                                                      (0 + 12) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-------+---------+------------------+\n",
      "|dur_days|dur_hours|dur_min|  dur_sec|       total_hours|\n",
      "+--------+---------+-------+---------+------------------+\n",
      "|       2|       18|     52|44.000000| 66.87888888888666|\n",
      "|       1|        1|     32|59.000000|25.549722222223334|\n",
      "|       0|       19|     58|51.000000|19.980833333336665|\n",
      "|       0|       18|     11|50.000000|18.197222222223335|\n",
      "|       0|       16|     28| 1.000000|16.466944444446664|\n",
      "+--------+---------+-------+---------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "WITH\n",
    "     query_dur AS (\n",
    "          SELECT\n",
    "               dropoff_datetime - pickup_datetime AS trip_duration\n",
    "          FROM\n",
    "               fhvhv_2021_06\n",
    "     ),\n",
    "     query_extracted AS (\n",
    "          SELECT\n",
    "               DATE_PART ('DAY', trip_duration) AS dur_days,\n",
    "               DATE_PART ('HOUR', trip_duration) AS dur_hours,\n",
    "               DATE_PART ('MINUTE', trip_duration) AS dur_min,\n",
    "               DATE_PART ('SECOND', trip_duration) AS dur_sec\n",
    "          FROM\n",
    "               query_dur\n",
    "     )\n",
    "SELECT\n",
    "     dur_days,\n",
    "     dur_hours,\n",
    "     dur_min,\n",
    "     dur_sec,\n",
    "     dur_days * 24 + dur_hours + dur_min / 60 + dur_sec / 3600 AS total_hours\n",
    "FROM\n",
    "     query_extracted\n",
    "ORDER BY\n",
    "     total_hours DESC\n",
    "LIMIT\n",
    "     5\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ee14f1-0fc3-484b-8b8f-c445ca783d50",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "**A**: port 4040\n",
    "\n",
    "http://localhost:4040/jobs/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9543a9a0-159a-4f8f-8266-16b90cb62d57",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "\n",
    "**Most frequent pickup location zone**<BR>\n",
    "Load the zone lookup data into a temp view in Spark<br>\n",
    "Using the zone lookup data and the fhvhv June 2021 data, what is the name of the most frequent pickup location zone?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "35d1a42e-7885-42a8-a6a6-55d7f92f8a11",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-03-04 11:54:12--  https://github.com/DataTalksClub/nyc-tlc-data/releases/download/misc/taxi_zone_lookup.csv\n",
      "Resolving github.com (github.com)... 20.201.28.151\n",
      "Connecting to github.com (github.com)|20.201.28.151|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/513814948/5a2cc2f5-b4cd-4584-9c62-a6ea97ed0e6a?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230304%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230304T145415Z&X-Amz-Expires=300&X-Amz-Signature=184f0e7e8aa102f5dac507e476dbf34f95196569baa22dbdf58d9b0e26c32094&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=513814948&response-content-disposition=attachment%3B%20filename%3Dtaxi_zone_lookup.csv&response-content-type=application%2Foctet-stream [following]\n",
      "--2023-03-04 11:54:13--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/513814948/5a2cc2f5-b4cd-4584-9c62-a6ea97ed0e6a?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230304%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230304T145415Z&X-Amz-Expires=300&X-Amz-Signature=184f0e7e8aa102f5dac507e476dbf34f95196569baa22dbdf58d9b0e26c32094&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=513814948&response-content-disposition=attachment%3B%20filename%3Dtaxi_zone_lookup.csv&response-content-type=application%2Foctet-stream\n",
      "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.110.133, ...\n",
      "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 12322 (12K) [application/octet-stream]\n",
      "Saving to: ‘/home/fromio/cursos/de-zoom-camp/week5/taxi_zone_lookup.csv’\n",
      "\n",
      "/home/fromio/cursos 100%[===================>]  12.03K  --.-KB/s    in 0.002s  \n",
      "\n",
      "2023-03-04 11:54:13 (6.91 MB/s) - ‘/home/fromio/cursos/de-zoom-camp/week5/taxi_zone_lookup.csv’ saved [12322/12322]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# downloading...\n",
    "zones_url = \"https://github.com/DataTalksClub/nyc-tlc-data/releases/download/misc/taxi_zone_lookup.csv\"\n",
    "local_path = \"/home/fromio/cursos/de-zoom-camp/week5/taxi_zone_lookup.csv\"\n",
    "\n",
    "!wget {zones_url} -O {local_path}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "3aefe030-eb09-4764-b820-acab776831d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- LocationID: string (nullable = true)\n",
      " |-- Borough: string (nullable = true)\n",
      " |-- Zone: string (nullable = true)\n",
      " |-- service_zone: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_zones = spark.read \\\n",
    "        .options(header=True) \\\n",
    "        .csv(local_path)\n",
    "\n",
    "df_zones.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "051ca781-d8e5-49f4-adaa-a50dc9e258a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------------------+------------+\n",
      "|LocationID|      Borough|                Zone|service_zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "|         1|          EWR|      Newark Airport|         EWR|\n",
      "|         2|       Queens|         Jamaica Bay|   Boro Zone|\n",
      "|         3|        Bronx|Allerton/Pelham G...|   Boro Zone|\n",
      "|         4|    Manhattan|       Alphabet City| Yellow Zone|\n",
      "|         5|Staten Island|       Arden Heights|   Boro Zone|\n",
      "|         6|Staten Island|Arrochar/Fort Wad...|   Boro Zone|\n",
      "|         7|       Queens|             Astoria|   Boro Zone|\n",
      "|         8|       Queens|        Astoria Park|   Boro Zone|\n",
      "|         9|       Queens|          Auburndale|   Boro Zone|\n",
      "|        10|       Queens|        Baisley Park|   Boro Zone|\n",
      "|        11|     Brooklyn|          Bath Beach|   Boro Zone|\n",
      "|        12|    Manhattan|        Battery Park| Yellow Zone|\n",
      "|        13|    Manhattan|   Battery Park City| Yellow Zone|\n",
      "|        14|     Brooklyn|           Bay Ridge|   Boro Zone|\n",
      "|        15|       Queens|Bay Terrace/Fort ...|   Boro Zone|\n",
      "|        16|       Queens|             Bayside|   Boro Zone|\n",
      "|        17|     Brooklyn|             Bedford|   Boro Zone|\n",
      "|        18|        Bronx|        Bedford Park|   Boro Zone|\n",
      "|        19|       Queens|           Bellerose|   Boro Zone|\n",
      "|        20|        Bronx|             Belmont|   Boro Zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "zones_schema = types.StructType([\n",
    " types.StructField(\"LocationID\", types.IntegerType(), True),\n",
    " types.StructField(\"Borough\", types.StringType(), True),\n",
    " types.StructField(\"Zone\", types.StringType(), True),\n",
    " types.StructField(\"service_zone\", types.StringType(), True) \n",
    " ])\n",
    "\n",
    "df_zones = spark.read \\\n",
    "        .options(header=True) \\\n",
    "        .schema(zones_schema) \\\n",
    "        .csv(local_path)\n",
    "\n",
    "df_zones.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "a1c635d2-82fb-4d15-ac9b-90068935607b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_zones.createOrReplaceTempView(\"taxi_zones\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "e9a70e4b-c9a9-459b-b2db-780aa6f999e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 153:=========>                                             (2 + 10) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+------+\n",
      "|               Zone|  Borough| total|\n",
      "+-------------------+---------+------+\n",
      "|Crown Heights North| Brooklyn|231279|\n",
      "|       East Village|Manhattan|221244|\n",
      "|        JFK Airport|   Queens|188867|\n",
      "|     Bushwick South| Brooklyn|187929|\n",
      "|      East New York| Brooklyn|186780|\n",
      "+-------------------+---------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "     zp.Zone,\n",
    "     zp.Borough,\n",
    "     COUNT(1) AS total\n",
    "FROM\n",
    "     fhvhv_2021_06 trips\n",
    "     JOIN taxi_zones zp ON trips.PULocationID = zp.LocationID\n",
    "GROUP BY\n",
    "     zp.Zone,\n",
    "     zp.Borough\n",
    "ORDER BY\n",
    "     total DESC\n",
    "LIMIT\n",
    "     5\n",
    "\"\"\") \\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964ca96f-e0cc-4780-a763-0845f35b0fef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
